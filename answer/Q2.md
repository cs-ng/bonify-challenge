# Q2
This is my take on the text clustering solution

The flowchart can be found [here][1]

First of all I think streaming is a good approach to tackle this as we're talking about files that we dont 
know how large it could be.

Procedures:
1. First whenever a new file is attached, a new stream instance will be initiated to read from the attached file.
Preferably to set a number of paralleism depending on the number of CPU cores the machine has, as a backpressure precaution
just in case that the consumer flows couldn't catch up to the file reading speed

2. Since the service accepts multiple file format, there should be a set of sanitizers for each file type. For example,
unwanted data like xml tags should be removed before proceeding further.

3. By referring to the diagram, there should be a flatten list of unique keywords prepared. Map over the flatten list and check
if that line of string contains each of the entry of the list. If String.contains() == true, then update the matched keyword map
and increasing the count by 1.

4. At the end of the stream, an accumulated map of matched string and the number of occurrences should be returned

5. Once we have the result, we can map over the original map of category and list of keywords, count the total number of
keyword occurrences of each category, then return the category that has the maximum count. Considering that keywords could exists
in more than 1 category.

[1]: https://drive.google.com/open?id=1me7DMUI6KW5-UdfnPeQAtWIrO0sIx_6u